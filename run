#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jun  3 13:54:12 2020

@author: qicai
"""

import time
import random
import numpy as np
from scipy.io import mmread
from scipy.sparse import coo_matrix
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F


import my_environment
import my_graph_library


num_graphs = 10
num_epi = 10 
size = 100
prob = 0.1
capacity = 10000


collect_new_data = 0
if collect_new_data:
    memory = my_environment.explore(num_graphs, num_epi, size, prob, capacity)
                
###############################################################################             
                
class DQN(nn.Module):

    def __init__(self):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(5, 5)
        self.fc2 = nn.Linear(5, 1)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return x                
                
                
policy_net = DQN()
target_net = DQN()
target_net.load_state_dict(policy_net.state_dict())
optimizer = optim.RMSprop(policy_net.parameters())

def optimize_model():
    transition = memory.sample(1)[0]
    graph, matching = transition.state
    edge = transition.action
    reward = transition.reward
    feature = (my_environment.env(graph, matching)).feature(edge)
    state_action_value = policy_net(torch.tensor(feature, dtype=torch.float32))
    
    if transition.next_state is None:
        next_state_value = torch.tensor([0], dtype=torch.float32)
    else:
        graph2, matching2 = transition.next_state
        available = (my_environment.env(graph2, matching2)).available_edges()
        new_env = my_environment.env(graph2, matching2)
        values = [target_net(torch.tensor(new_env.feature(edge), dtype=torch.float32)) for edge in available]
        next_state_value = max(values).detach()
        
        
    expected_state_action_value = next_state_value + reward  
    loss = F.smooth_l1_loss(state_action_value, expected_state_action_value)

    optimizer.zero_grad()
    loss.backward()
    #for param in policy_net.parameters():
    #    param.grad.data.clamp_(-1, 1)
    optimizer.step()
    return


train_model = 0
if train_model:
    total_iter_num = 2000
    for j in range(total_iter_num):
        if j%100 == 0:
            percent = str(round(j/total_iter_num * 100)) + '%'
            print('process: ' + percent)
        optimize_model()
        target_net.load_state_dict(policy_net.state_dict())


###############################################################################


graph_names = ['lp_e226.mtx',
 'bcspwr10.mtx',
 'poli.mtx',
 'msc01440.mtx',
 'G17.mtx',
 'G15.mtx',
 'bcspwr01.mtx',
 'dwt_198.mtx',
 'bcsstk05.mtx',
 'lshp_406.mtx',
 'can_62.mtx',
 '662_bus.mtx',
 'dwt_72.mtx',
 'b2_ss.mtx',
 'sphere3.mtx',
 'dwt_2680.mtx',
 'mark3jac020sc.mtx',
 'bayer04.mtx'
]

optimal_soln = [200,2576,792,720,400,400,17,99,76,203,29,306,32,544,129,1340,4564,10272]

matching_results = []


for i in range(18):
    name = graph_names[i]
    print("graph name: {0:20}".format(name), end = '')
    
    mtx = mmread('graphs_collection/' + name)
    length = max(mtx.shape[0], mtx.shape[1])
    mtx.resize(length, length)
    mtx = coo_matrix(mtx + mtx.transpose())
    
    graph = my_graph_library.graph_diy(mtx)
    environment = my_environment.env(graph, set())
    if len(environment.avail_edges) == 0:
        matching_results.append(0)
        continue
    
    while True:
        available = environment.avail_edges
        choice_size = min(len(available),10)
        
        random_search = 0
        if random_search:
            action = random.sample(available,1)[0]
        else:
            available = random.sample(available, choice_size)
            values = [policy_net(torch.tensor(environment.feature(edge), dtype=torch.float32)) for edge in available]
            indice = np.argmax(np.array(values), 0)
            action = available[indice]
        
        environment.step(action) 
        
        
        if len(environment.avail_edges) == 0:
            break
            
    matching = environment.matching    
    cardi = len(matching)
    gap = (optimal_soln[i]-cardi)/optimal_soln[i]
    gap_p = str(round(gap*100))+'%'
    print("matching cardinality: {0:8}  optimality gap: {1}".format(str(cardi), gap_p))
    matching_results.append([cardi, gap])


average_gap = np.array(matching_results)[:,1].mean()
print('average optimality gap: ' + str(average_gap))









               
                
                
                
                
                
                